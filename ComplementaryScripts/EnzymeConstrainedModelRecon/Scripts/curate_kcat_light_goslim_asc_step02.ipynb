{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrain based on go-term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "from multiprocessing import Process,cpu_count,Manager\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecpy_path = '../../../ecpy/'\n",
    "sys.path.append(os.path.abspath(ecpy_path))\n",
    "import utils\n",
    "import ecpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(ecpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RV:\n",
    "    def __init__(self,loc,scale):\n",
    "        '''\n",
    "        Normal distribution assumed\n",
    "        loc, scale: the same as used in scipy.stats. kcat in log10(1/h)\n",
    "        \n",
    "        '''\n",
    "       \n",
    "        \n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.lb = np.log10(1e-5*3600) # kcat in range 1e-5, 1e6 1/s\n",
    "        self.ub = np.log10(1e6*3600)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Generate a random sample from the given prior distribution\n",
    "        '''\n",
    "        kcat = np.random.normal(self.loc,self.scale)\n",
    "        if kcat < self.lb: kcat = self.lb\n",
    "        if kcat > self.ub: kcat = self.ub\n",
    "        return kcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_kcats_dict(tmp_ecmodel):\n",
    "    # Initialized kcats_dict, h-1, values do not matter since only keys will be used \n",
    "    initial_kcats = {}\n",
    "    for met in tmp_ecmodel.metabolites:\n",
    "        # look for those metabolites: prot_protid\n",
    "        if not met.id.startswith('prot_'): continue\n",
    "\n",
    "        # ingore metabolite: prot_pool\n",
    "        if met.id.startswith('prot_pool'): continue\n",
    "\n",
    "        prot_id = met.id.split('_')[1]\n",
    "        for rxn in met.reactions:\n",
    "            if rxn.id.startswith('draw_prot'): continue\n",
    "            initial_kcats[(rxn.id,met.id)] = -1/rxn.metabolites[met]\n",
    "    return initial_kcats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_priors(tmp_ecmodel,df_enz_kcat):\n",
    "    # tmp_ecmodel, an ecModel with all enzymes and pools\n",
    "    # in df_enz_kcat, value are in the unit of 1/s, log1o-transformed\n",
    "    # in the resulting prior, kcat is in log10-transformed, 1/h\n",
    "    \n",
    "    initial_kcats = initialize_kcats_dict(tmp_ecmodel)\n",
    "            \n",
    "    # Build priors\n",
    "    priors = dict()\n",
    "    for rxn_id,prot_met_id in initial_kcats.keys():\n",
    "        rxn_short_id = rxn_id.split('No')[0]\n",
    "        loc = np.log10(10**df_enz_kcat.loc[rxn_short_id,'log10_kcat_mean']*3600)\n",
    "        std_in_log_seconds = df_enz_kcat.loc[rxn_short_id,'log10_kcat_std']\n",
    " \n",
    "        scale = np.log10(10**std_in_log_seconds*3600)\n",
    "        priors[(rxn_id,prot_met_id)] = RV(loc,scale)\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateProteinConstraints(tmp_ecmodel,pools):\n",
    "    # pools, a dictionary, {'prot_pool_exchange_GO0006457': ub, ...}\n",
    "    for rxn_id, ub in pools.items():\n",
    "        rxn = tmp_ecmodel.reactions.get_by_id(rxn_id)\n",
    "        rxn.upper_bound = ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildPoolDict(dffrac,condition_id,Ptot,sigma=0.5):\n",
    "    # usego = True  : build go based pools\n",
    "    # usego = False : build only one pool\n",
    "    one_pool = dict()\n",
    "    go_pools = dict()\n",
    "    \n",
    "    one_pool['prot_pool_exchange'] = np.sum(dffrac['MassFrac_{0}'.format(condition_id)]) * Ptot * sigma\n",
    "    \n",
    "    for go in dffrac.index:\n",
    "        rxn_id = 'prot_pool_exchange_{0}'.format(go.replace(':',''))\n",
    "        go_pools[rxn_id] = dffrac.loc[go,'MassFrac_{0}'.format(condition_id)] * Ptot * sigma\n",
    "        \n",
    "    return one_pool,go_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateKcats(ecmodel,kcats):\n",
    "    # kcats: dict() {(rxn_id, prot_met_id): kcat in h-1}\n",
    "    for (rxn_id, prot_met_id),kcat in kcats.items():\n",
    "        rxn = ecmodel.reactions.get_by_id(rxn_id)\n",
    "        prot_met = ecmodel.metabolites.get_by_id(prot_met_id)\n",
    "        \n",
    "        new_coeff = -1./kcat\n",
    "        \n",
    "        ecpy.change_rxn_coeff(rxn,prot_met,new_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model,pools,tag=''):\n",
    "    model.objective = 'Biomass_v1'\n",
    "    model.objective_direction = 'max'\n",
    "    s = model.optimize()\n",
    "    r = s.objective_value if s.status == 'optimal' else -1\n",
    "    print('Results {0}:'.format(tag))\n",
    "    print('  Status        :',s.status)\n",
    "    print('  growth_rate   :',r)\n",
    "    print('  glucose uptake:',s.fluxes['Exchange_Glucopyranose'])\n",
    "    print('  PHA           :',s.fluxes['PHA_secretion'])\n",
    "    print('  NGAM:',model.reactions.NGAM.lower_bound)\n",
    "    print('   PHA:',model.reactions.PHA_secretion.lower_bound)\n",
    "    for pool_rxn in pools.keys():\n",
    "        print('  Protein pool  :',model.reactions.get_by_id(pool_rxn).upper_bound)\n",
    "    print()\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_growth(model,pools,dfpheno,kcats,condition_id,tag=''):\n",
    "    # kcats: dict() {(rxn_id, prot_met_id): kcat in h-1}\n",
    "    # return two growth rates: one for model with NGAM and PHA constraints and one for without\n",
    "    \n",
    "    with model:\n",
    "        updateProteinConstraints(model,pools)\n",
    "        updateKcats(model,kcats)\n",
    "        \n",
    "        PHA = dfpheno.loc[condition_id,'P3HB']\n",
    "        if ~np.isnan(PHA): \n",
    "            model.reactions.PHA_secretion.lower_bound = PHA\n",
    "        \n",
    "        try: \n",
    "            r1 = optimize_model(model,pools,'with pha ngam') \n",
    "            model.reactions.PHA_secretion.lower_bound = 0\n",
    "            model.reactions.NGAM.lower_bound = 0\n",
    "            \n",
    "            r2 = optimize_model(model,pools,'without pha ngam')\n",
    "        except: r1,r2 = -1,-1\n",
    "    return [r1,r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(#model_one_pool,      # json format\n",
    "               model_go_pools,\n",
    "               kcats,          # kcats: dict() {(rxn_id, prot_met_id): kcat in h-1}\n",
    "               dftot,          # contains the total protein abandance, in the unit of gram protein/gram CDW\n",
    "               dfpheno,        # contains the P3HB specific synthetic rate\n",
    "               dffrac,\n",
    "               condition_id,   # condition id\n",
    "                ):\n",
    "    \n",
    "    \n",
    "    # constrain protein pool\n",
    "    print('Condition:',condition_id)\n",
    "    Ptot = dftot.loc[condition_id,'Ptot']\n",
    "    one_pool,go_pools = buildPoolDict(dffrac,condition_id,Ptot)\n",
    "    \n",
    "    #r1 = simulate_growth(model_one_pool,one_pool,dfpheno,kcats,condition_id,tag='one pool')\n",
    "    r2 = simulate_growth(model_go_pools,go_pools,dfpheno,kcats,condition_id,tag='go pools')\n",
    "    \n",
    "    print(''.join(['\\n']*2))\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_a_single_kcat_set_on_all_datasets(datasets,kcats):\n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    #model_one_pool = pickle.load(open(datasets['model_one_pool_file'],'rb'))\n",
    "    model_go_pools = pickle.load(open(datasets['model_go_pools_file'],'rb'))\n",
    "    \n",
    "    datasets_copy = datasets.copy()\n",
    "    #datasets_copy.pop('model_one_pool_file')\n",
    "    datasets_copy.pop('model_go_pools_file')\n",
    "    \n",
    "    for condition_id in datasets['dfpheno'].index:\n",
    "        datasets_copy['condition_id'] = condition_id\n",
    "        res = test_model(model_go_pools,kcats=kcats,**datasets_copy)\n",
    "        results[condition_id] = res\n",
    "    print('Time:',time.time() - start_time)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smc_abc:\n",
    "    def __init__(self,datasets,priors,epsilon,outfile,cores=cpu_count(),population_size=100):\n",
    "        self.datasets   = datasets\n",
    "        self.population = []\n",
    "        self.epsilons   = [np.inf] # store the distance after each generation\n",
    "        self.prior      = priors.copy()\n",
    "        self.posterior  = priors.copy() # a dictionary with rxn_id as key, RV as value\n",
    "        self.cores      = cores\n",
    "        \n",
    "        self.outfile    = outfile\n",
    "        self.population = []  # a list of populations [p1,p2...]\n",
    "        self.distances  = []    # a list of distances for particles in population\n",
    "        self.simulations = 0  # number of simulations performed \n",
    "        self.simulated_data = []\n",
    "        \n",
    "        self.population_size = population_size\n",
    "        \n",
    "        #self.all_simulated_particle = []\n",
    "        #self.all_simulated_data     = []\n",
    "        #self.all_simulated_distance = []\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def simulate_one(self,particle,index,Q):\n",
    "        '''\n",
    "        particle:  parameters \n",
    "        Q:      a multiprocessing.Queue object\n",
    "        index:  the index in particles list\n",
    "        '''\n",
    "        \n",
    "        res = simulate_a_single_kcat_set_on_all_datasets(self.datasets,particle)\n",
    "\n",
    "        Q.put((index,res))\n",
    "    \n",
    "    def distance(self,res):\n",
    "        y_sim = []\n",
    "        y_exp = []\n",
    "        for condition_id,lst in res.items():\n",
    "            y_sim.extend(lst)\n",
    "            y_exp.extend([self.datasets['dfpheno'].loc[condition_id,'SpecificGrowthRate']]*len(lst))\n",
    "            \n",
    "        return mean_squared_error(y_exp,y_sim)\n",
    "        \n",
    "    def calculate_distances_parallel(self,particles):\n",
    "        Q = Manager().Queue()\n",
    "        jobs = [Process(target=self.simulate_one,args=(particle,index,Q)) \n",
    "                               for index,particle in enumerate(particles)]\n",
    "        \n",
    "        for p in jobs: p.start()\n",
    "        for p in jobs: p.join()\n",
    "        \n",
    "        distances = [None for _ in range(len(particles))]\n",
    "        simulated_data = [None for _ in range(len(particles))]\n",
    "\n",
    "        for index,res in [Q.get(timeout=1) for p in jobs]: \n",
    "            distances[index] = self.distance(res)\n",
    "            simulated_data[index] = res\n",
    "        \n",
    "        # save all simulated results\n",
    "        #self.all_simulated_data.extend(simulated_data)\n",
    "        #self.all_simulated_distance.extend(distances)\n",
    "        #self.all_simulated_particle.extend(particles)\n",
    "        \n",
    "        del Q, jobs\n",
    "        return distances,simulated_data\n",
    "        \n",
    "    def simulate_a_generation(self):\n",
    "        particles_t, simulated_data_t, distances_t = [], [], []\n",
    "        while len(particles_t) < self.population_size:\n",
    "            self.simulations += self.cores\n",
    "            particles = [{idp: 10**rv.sample() for idp,rv in self.posterior.items()} for i in range(self.cores)]\n",
    "\n",
    "            distances,simulated_data = self.calculate_distances_parallel(particles)\n",
    "            \n",
    "            particles_t.extend(particles)\n",
    "            simulated_data_t.extend(simulated_data)\n",
    "            distances_t.extend(distances)\n",
    "        \n",
    "        return particles_t, simulated_data_t, distances_t\n",
    "    \n",
    "    def update_population(self,particles_t, simulated_data_t, distances_t):\n",
    "        print ('updating population')\n",
    "        # save first generation\n",
    "        if len(self.population) == 0:\n",
    "            self.population_t0 = particles_t.copy()\n",
    "            self.distances_t0 = distances_t.copy()\n",
    "            self.simulated_data_t0 = simulated_data_t.copy()\n",
    "        \n",
    "        \n",
    "        combined_particles = np.array(self.population + particles_t)\n",
    "        combined_distances = np.array(self.distances + distances_t)\n",
    "        combined_simulated = np.array(self.simulated_data + simulated_data_t)\n",
    "        \n",
    "        sort_index = np.argsort(combined_distances)\n",
    "        self.population = list(combined_particles[sort_index][:self.population_size])\n",
    "        self.distances = list(combined_distances[sort_index][:self.population_size])\n",
    "        self.simulated_data = list(combined_simulated[sort_index][:self.population_size])\n",
    "        self.epsilons.append(np.max(self.distances))\n",
    "        \n",
    "        print('Model: epsilon=',str(self.epsilons[-1]))\n",
    "        \n",
    "    def update_posterior(self):\n",
    "        print ('Updating prior')\n",
    "        parameters = dict()   # {'':[]}\n",
    "        for particle in self.population:\n",
    "            for p,val in particle.items(): \n",
    "                lst = parameters.get(p,[])\n",
    "                lst.append(np.log10(val))\n",
    "                parameters[p] =lst\n",
    "        \n",
    "        for p, lst in parameters.items():\n",
    "            self.posterior[p] = RV(loc = np.mean(lst), scale = np.std(lst))\n",
    "        \n",
    "        \n",
    "    def run_simulation(self):\n",
    "        while self.epsilons[-1] > self.epsilon:\n",
    "            particles_t, simulated_data_t, distances_t = self.simulate_a_generation()\n",
    "            self.update_population(particles_t, simulated_data_t, distances_t)\n",
    "            self.update_posterior()\n",
    "            pickle.dump(self,open(self.outfile,'wb'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model_one_pool_file = '../Results/template_ecModel_one_pool.pkl'\n",
    "    model_go_pools_file = '../Results/template_ecModel_goslim_pools.pkl'\n",
    "    \n",
    "    dffrac = pd.read_csv('../Results/protein_abundance_go_slim_level_uniq_asc.csv',index_col=0)\n",
    "    dftot = pd.read_csv('../proteomics/total_protein_abandance_mean.csv',index_col=0)\n",
    "    dfpheno = pd.read_csv('../proteomics/phynotype.csv',index_col=0,comment='#')\n",
    "    df_enz_kcat = pd.read_csv('../Results/mapped_kcats_updated_with_ko.csv',index_col=0)\n",
    "    \n",
    "    datasets = {\n",
    "        #'model_one_pool_file': model_one_pool_file,\n",
    "        'model_go_pools_file': model_go_pools_file,\n",
    "        'dffrac'      : dffrac,\n",
    "        'dfpheno'     : dfpheno,\n",
    "        'dftot'       : dftot,\n",
    "    }\n",
    "    \n",
    "    tmp_ecmodel = pickle.load(open(model_one_pool_file,'rb'))\n",
    "    priors = build_priors(tmp_ecmodel,df_enz_kcat)\n",
    "    \n",
    "    \n",
    "    epsilon = 0\n",
    "    outfile = '../Results/smc_abc_light_step_02.pkl'\n",
    "    if os.path.isfile(outfile):\n",
    "        experiment = pickle.load(open(outfile,'rb'))\n",
    "    else:\n",
    "        experiment = smc_abc(datasets,priors,epsilon,outfile)\n",
    "    experiment.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def run_test():\n",
    "    #model_one_pool_file = '../Results/template_ecModel_one_pool.pkl'\n",
    "    model_go_pools_file = '../Results/template_ecModel_goslim_pools.pkl'\n",
    "    \n",
    "    dffrac = pd.read_csv('../Results/protein_abundance_go_slim_level_uniq_asc.csv',index_col=0)\n",
    "    dftot = pd.read_csv('../proteomics/total_protein_abandance_mean.csv',index_col=0)\n",
    "    dfpheno = pd.read_csv('../proteomics/phynotype.csv',index_col=0,comment='#')\n",
    "    df_enz_kcat = pd.read_csv('../Results/mapped_kcats_updated_with_ko.csv',index_col=0)\n",
    "    \n",
    "    datasets = {\n",
    "        #'model_one_pool_file': model_one_pool_file,\n",
    "        'model_go_pools_file': model_go_pools_file,\n",
    "        'dffrac'      : dffrac,\n",
    "        'dfpheno'     : dfpheno,\n",
    "        'dftot'       : dftot,\n",
    "    }\n",
    "    \n",
    "    step1_outfile = '../Results/smc_abc_light_go_and_one_asc_step_1.pkl'\n",
    "    step1_res = pickle.load(open(step1_outfile,'rb'))\n",
    "    priors = step1_res.posterior\n",
    "    #tmp_ecmodel = pickle.load(open(model_one_pool_file,'rb'))\n",
    "    #priors = build_priors(tmp_ecmodel,df_enz_kcat)\n",
    "    \n",
    "    \n",
    "    epsilon = 0\n",
    "    outfile = '../Results/smc_abc_light_go_and_one_asc_step_2_test.pkl'\n",
    "    experiment = smc_abc(datasets,priors,epsilon,outfile,cores=4,population_size=4)\n",
    "    experiment.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_kcats_test(tmp_ecmodel,df_enz_kcat):\n",
    "    # tmp_ecmodel, an ecModel with all enzymes and pools\n",
    "    # in df_enz_kcat, value are in the unit of 1/s, log1o-transformed\n",
    "    # in the resulting prior, kcat is in log10-transformed, 1/h\n",
    "    \n",
    "    initial_kcats = initialize_kcats_dict(tmp_ecmodel)\n",
    "    \n",
    "    # Build priors\n",
    "    kcats = dict()\n",
    "    for rxn_id,prot_met_id in initial_kcats.keys():\n",
    "        rxn_short_id = rxn_id.split('No')[0]\n",
    "        kcats[(rxn_id,prot_met_id)] = 10**df_enz_kcat.loc[rxn_short_id,'log10_kcat_mean']*3600\n",
    "    return kcats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp_ecmodel.reactions.get_by_id('RXN-12565No1').reaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    model_one_pool_file = '../Results/template_ecModel_one_pool.pkl'\n",
    "    #model_go_pools_file = '../Results/template_ecModel_goslim_pools.pkl'\n",
    "    \n",
    "    dffrac = pd.read_csv('../Results/protein_abundance_go_slim_level_uniq_asc.csv',index_col=0)\n",
    "    dftot = pd.read_csv('../proteomics/total_protein_abandance_mean.csv',index_col=0)\n",
    "    dfpheno = pd.read_csv('../proteomics/phynotype.csv',index_col=0,comment='#')\n",
    "    df_enz_kcat = pd.read_csv('../Results/mapped_kcats_updated_with_ko.csv',index_col=0)\n",
    "    \n",
    "    datasets = {\n",
    "        'model_one_pool_file': model_one_pool_file,\n",
    "        #'model_go_pools_file': model_go_pools_file,\n",
    "        'dfpheno'     : dfpheno,\n",
    "        'dftot'       : dftot,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    tmp_ecmodel = pickle.load(open(model_one_pool_file,'rb'))\n",
    "    kcats=load_kcats_test(tmp_ecmodel,df_enz_kcat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
