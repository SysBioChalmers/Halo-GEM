{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "from multiprocessing import Process,cpu_count,Manager\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecpy_path = '../../../ecpy/'\n",
    "sys.path.append(os.path.abspath(ecpy_path))\n",
    "import utils\n",
    "import ecpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ecpy' from '/Users/gangl/Documents/GitHub/Halo-GEM/ecpy/ecpy.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(ecpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_max(model):\n",
    "    model.objective = 'Biomass_v1'\n",
    "    model.objective_direction = 'max'\n",
    "    return model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(modelfile,      # json format\n",
    "               kcats,          # dict() with enzyme kcats, in the unit of 1/h\n",
    "               dfmws,          # dataframe with all protein weights, in the unit of kDa\n",
    "               dfomics,        # contains absolute proteomics data\n",
    "               dftot,          # contains the total protein abandance, in the unit of gram protein/gram CDW\n",
    "               dfpheno,        # contains the P3HB specific synthetic rate\n",
    "               condition_id,   # condition id\n",
    "               sigma=0.5,\n",
    "               huge_kcat=False # use huge kcat to release the kcat constraint\n",
    "                ):\n",
    "    \n",
    "    print('Condition:',condition_id)\n",
    "    model = cobra.io.load_json_model(modelfile)\n",
    "    irrModel = ecpy.convertToIrrev(model)\n",
    "    if huge_kcat: \n",
    "        for k,v in kcats.items(): kcats[k] = 1e8\n",
    "            \n",
    "    eModel = ecpy.convertToEnzymeModel(irrModel,kcats)\n",
    "    \n",
    "    # with proteomics\n",
    "    measured, non_measured, prot_pool, enz_frac = ecpy.prepare_omics_for_one_condition(dfomics,\n",
    "                                                                             dftot,\n",
    "                                                                             dfmws,\n",
    "                                                                             condition_id,\n",
    "                                                                             eModel.enzymes)\n",
    "    MWs = {ind:dfmws.loc[ind,'MW'] for ind in dfmws.index}\n",
    "    \n",
    "    with eModel:\n",
    "        ecModel = ecpy.constrainPool(eModel,MWs, measured, non_measured,prot_pool*sigma)\n",
    "\n",
    "        PHA = dfpheno.loc[condition_id,'P3HB']\n",
    "        if ~np.isnan(PHA): \n",
    "            ecModel.reactions.PHA_secretion.lower_bound = PHA\n",
    "\n",
    "        s1 = get_r_max(ecModel)\n",
    "        r1 = s1.objective_value if s1.status == 'feasible' else -1\n",
    "        print('Case 1: with omics constraints')\n",
    "        print('  Status        :',s1.status)\n",
    "        print('  growth_rate   :',r1)\n",
    "        print('  glucose uptake:',s1.fluxes['Exchange_Glucopyranose'])\n",
    "        print('  PHA           :',s1.fluxes['PHA_secretion'])\n",
    "        print('  Protein pool  :',ecModel.reactions.get_by_id('prot_pool_exchange').upper_bound)\n",
    "        print()\n",
    "\n",
    "        \n",
    "    # without proteomics\n",
    "    with eModel:\n",
    "        ecModel = ecpy.constrainPool(eModel,MWs, {}, eModel.enzymes,\n",
    "                                     dftot.loc[condition_id,'Ptot']*enz_frac*sigma)\n",
    "        if ~np.isnan(PHA): \n",
    "            ecModel.reactions.PHA_secretion.lower_bound = PHA\n",
    "\n",
    "        s2 = get_r_max(ecModel)\n",
    "        r2 = s2.objective_value if s2.status == 'feasible' else -1\n",
    "        print('Case 2: without omics constraints')\n",
    "        print('  Status        :',s2.status)\n",
    "        print('  growth_rate   :',r2)\n",
    "        print('  glucose uptake:',s2.fluxes['Exchange_Glucopyranose'])\n",
    "        print('  PHA           :',s2.fluxes['PHA_secretion'])\n",
    "        print('  Protein pool  :',ecModel.reactions.get_by_id('prot_pool_exchange').upper_bound)\n",
    "\n",
    "    print(''.join(['\\n']*2))\n",
    "    return [r1,r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_a_single_kcat_set_on_all_datasets(datasets,kcats):\n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    for condition_id in datasets['dfpheno'].index:\n",
    "        datasets['condition_id'] = condition_id\n",
    "        res = test_model(kcats=kcats,**datasets)\n",
    "        results[condition_id] = res\n",
    "    print('Time:',time.time() - start_time)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RV:\n",
    "    def __init__(self,loc,scale):\n",
    "        '''\n",
    "        Normal distribution assumed\n",
    "        loc, scale: the same as used in scipy.stats\n",
    "        \n",
    "        Use truncated normal distribution for kcat to ensure that all kcats are in (5.83e-10, 7900000.0) 1/s\n",
    "        '''\n",
    "       \n",
    "        \n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        \n",
    "        myclip_a = np.log10(1e-11 * 3600)\n",
    "        my_mean = loc\n",
    "        my_std = scale\n",
    "        \n",
    "        myclip_b = np.log10(1e7*3600)\n",
    "        \n",
    "        a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std\n",
    "        \n",
    "\n",
    "        self.rvf = ss.truncnorm(a,b,loc=my_mean,scale=my_std)\n",
    "        \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Generate a random sample from the given prior distribution\n",
    "        '''\n",
    "        return self.rvf.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smc_abc:\n",
    "    def __init__(self,datasets,priors,epsilon,outfile):\n",
    "        self.datasets   = datasets\n",
    "        self.population = []\n",
    "        self.epsilons   = [np.inf] # store the distance after each generation\n",
    "        self.prior      = priors\n",
    "        self.posterior  = priors # a dictionary with rxn_id as key, RV as value\n",
    "        self.cores      = cpu_count()\n",
    "        \n",
    "        self.outfile    = outfile\n",
    "        self.population = []  # a list of populations [p1,p2...]\n",
    "        self.distances  = []    # a list of distances for particles in population\n",
    "        self.simulations = 0  # number of simulations performed \n",
    "        \n",
    "        self.population_size = 100\n",
    "        self.generation_size = 128\n",
    "        \n",
    "        self.all_simulated_particle = []\n",
    "        self.all_simulated_data     = []\n",
    "        self.all_simulated_distance = []\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def simulate_one(self,particle,index,Q):\n",
    "        '''\n",
    "        particle:  parameters \n",
    "        Q:      a multiprocessing.Queue object\n",
    "        index:  the index in particles list\n",
    "        '''\n",
    "        \n",
    "        res = simulate_a_single_kcat_set_on_all_datasets(self.datasets,particle)\n",
    "\n",
    "        Q.put((index,res))\n",
    "    \n",
    "    def distance(self,res):\n",
    "        y_sim = []\n",
    "        y_exp = []\n",
    "        for condition_id,lst in res.items():\n",
    "            y_sim.extend(lst)\n",
    "            y_exp.extend([self.datasets['dfpheno'].loc[condition_id,'SpecificGrowthRate']]*2)\n",
    "            \n",
    "        return mean_squared_error(y_exp,y_sim)\n",
    "        \n",
    "    def calculate_distances_parallel(self,particles):\n",
    "        Q = Manager().Queue()\n",
    "        jobs = [Process(target=self.simulate_one,args=(particle,index,Q)) \n",
    "                               for index,particle in enumerate(particles)]\n",
    "        \n",
    "        for p in jobs: p.start()\n",
    "        for p in jobs: p.join()\n",
    "        \n",
    "        distances = [None for _ in range(len(particles))]\n",
    "        simulated_data = [None for _ in range(len(particles))]\n",
    "\n",
    "        for index,res in [Q.get(timeout=1) for p in jobs]: \n",
    "            distances[index] = self.distance(res)\n",
    "            simulated_data[index] = res\n",
    "        \n",
    "        # save all simulated results\n",
    "        self.all_simulated_data.extend(simulated_data)\n",
    "        self.all_simulated_distance.extend(distances)\n",
    "        self.all_simulated_particle.extend(particles)\n",
    "        \n",
    "        return distances,simulated_data\n",
    "        \n",
    "    def simulate_a_generation(self):\n",
    "        particles_t, simulated_data_t, distances_t = [], [], []\n",
    "        while len(particles_t) < self.generation_size:\n",
    "            self.simulations += self.cores\n",
    "            particles = [{idp: 10**rv.sample() for idp,rv in self.posterior.items()} for i in range(self.cores)]\n",
    "\n",
    "            distances,simulated_data = self.calculate_distances_parallel(particles)\n",
    "            \n",
    "            particles_t.extend(particles)\n",
    "            simulated_data_t.extend(simulated_data)\n",
    "            distances_t.extend(distances)\n",
    "        \n",
    "        return particles_t, simulated_data_t, distances_t\n",
    "    \n",
    "    def update_population(self,particles_t, simulated_data_t, distances_t):\n",
    "        print ('updating population')\n",
    "        # save first generation\n",
    "        if len(self.population) == 0:\n",
    "            self.population_t0 = particles_t\n",
    "            self.distances_t0 = distances_t\n",
    "            self.simulated_data_t0 = simulated_data_t\n",
    "        \n",
    "        \n",
    "        combined_particles = np.array(self.population + particles_t)\n",
    "        combined_distances = np.array(self.distances + distances_t)\n",
    "        combined_simulated = np.array(self.simulated_data + simulated_data_t)\n",
    "        \n",
    "        sort_index = np.argsort(combined_distances)\n",
    "        self.population = list(combined_particles[sort_index][:self.population_size])\n",
    "        self.distances = list(combined_distances[sort_index][:self.population_size])\n",
    "        self.simulated_data = list(combined_simulated[sort_index][:self.population_size])\n",
    "        self.epsilons.append(np.max(self.distances))\n",
    "        \n",
    "        print('Model: epsilon=',str(self.epsilons[-1]))\n",
    "        \n",
    "    def update_posterior(self):\n",
    "        print ('Updating prior')\n",
    "        parameters = dict()   # {'Protein_Tm':[]}\n",
    "        for particle in self.population:\n",
    "            for p,val in particle.items(): \n",
    "                lst = parameters.get(p,[])\n",
    "                lst.append(np.log10(val))\n",
    "                parameters[p] =lst\n",
    "        \n",
    "        for p, lst in parameters.items():\n",
    "            self.posterior[p] = RV(loc = np.mean(lst), scale = np.std(lst))\n",
    "        \n",
    "        \n",
    "    def run_simulation(self):\n",
    "        while self.epsilons[-1] > self.epsilon:\n",
    "            particles_t, simulated_data_t, distances_t = self.simulate_a_generation()\n",
    "            self.update_population(particles_t, simulated_data_t, distances_t)\n",
    "            self.update_posterior()\n",
    "            pickle.dump(self,open(self.outfile,'wb'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_priors(kcats,df_enz_kcat):\n",
    "    # in prior, kcat is in log10-transformed, 1/h\n",
    "    # in kcats, only keys will be used\n",
    "    # in df_enz_kcat, value are in the unit of 1/s, log1o-transformed\n",
    "    \n",
    "    priors = dict()\n",
    "    for (enz_id,rxn_id) in kcats.keys():\n",
    "        loc   = np.log10(10**df_enz_kcat.loc[rxn_id,'log10_kcat_mean']*3600)\n",
    "        scale = np.log10(10**df_enz_kcat.loc[rxn_id,'log10_kcat_std']*3600)\n",
    "        priors[(enz_id,rxn_id)] = RV(loc,scale)\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = '../../../ModelFiles/json/Halo_GEM_v1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfomics = pd.read_csv('../proteomics/protein_abandance_mean.csv',index_col=0)\n",
    "dftot = pd.read_csv('../proteomics/total_protein_abandance_mean.csv',index_col=0)\n",
    "dfmws = pd.read_csv('../Results/protein_mws.csv',index_col=0)\n",
    "dfmws = dfmws/1000\n",
    "dfpheno = pd.read_csv('../proteomics/phynotype.csv',index_col=0,comment='#')\n",
    "df_enz_kcat = pd.read_csv('../Results/mapped_kcats.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cobra.io.load_json_model(modelfile)\n",
    "irrModel = ecpy.convertToIrrev(model)\n",
    "kcats = ecpy.prepare_kcats_dict(irrModel,df_enz_kcat,'log10_kcat_mean')\n",
    "del model,irrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'modelfile': modelfile,\n",
    "    'dfomics'   : dfomics,\n",
    "    'dfmws'     : dfmws,\n",
    "    'dfpheno'     : dfpheno,\n",
    "    'dftot'       : dftot,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = build_priors(kcats,df_enz_kcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0\n",
    "outfile = '../Results/smc_abc.pkl'\n",
    "experiment = smc_abc(datasets,priors,epsilon,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = simulate_a_single_kcat_set_on_all_datasets(datasets,kcats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for condition_id in dfomics.columns:\n",
    "    if condition_id != 'NACL60': continue\n",
    "    test_model(modelfile,      # json format\n",
    "               df_enz_kcat,    # dataframe with enzyme kcats, in the unit of 1/s, log10transformed\n",
    "               'log10_kcat_mean',   # column name for kcat\n",
    "               dfmws,          # dataframe with all protein weights, in the unit of kDa\n",
    "               dfomics,        # contains absolute proteomics data\n",
    "               dftot,          # contains the total protein abandance, in the unit of gram protein/gram cell dry mass\n",
    "               dfpheno,        # contains the P3HB specific synthetic rate\n",
    "               condition_id,   # condition id\n",
    "               sigma=0.5,\n",
    "               huge_kcat=False )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "for condition_id in dfomics.columns:\n",
    "    print(condition_id)\n",
    "    if condition_id != 'NACL60': continue\n",
    "    test_model(modelfile,      # json format\n",
    "               df_enz_kcat,    # dataframe with enzyme kcats, in the unit of 1/s, log10transformed\n",
    "               'log10_kcat_mean',   # column name for kcat\n",
    "               dfmws,          # dataframe with all protein weights, in the unit of kDa\n",
    "               dfomics,        # contains absolute proteomics data\n",
    "               dftot,          # contains the total protein abandance, in the unit of gram protein/gram cell dry mass\n",
    "               dfpheno,        # contains the P3HB specific synthetic rate\n",
    "               condition_id,   # condition id\n",
    "               sigma=0.5,\n",
    "               huge_kcat=True )\n",
    "    print()\n",
    "print('Time:',time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
